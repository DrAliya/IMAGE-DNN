# -*- coding: utf-8 -*-
"""2.VENV-MultipleSplit-Images.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1SQmIhSnbcRPNP0KSiUuwJnEwZMj66kDS
"""

!pip install virtualenv

!virtualenv myenv

!source myenv/bin/activate

import os
import tensorflow as tf
import numpy as np
import pandas as pd

from sklearn.model_selection import StratifiedShuffleSplit
import tensorflow as tf

(x_train, y_train), (x_test, y_test) = tf.keras.datasets.cifar10.load_data()

# Creating StratifiedShuffleSplit object
stratified_split = StratifiedShuffleSplit(n_splits=4, test_size=0.2, random_state=42)

# Splitting the data into four sets
splits = list(stratified_split.split(x_train, y_train))

# Extracting the indices for each split
indices_train1, indices_test1 = splits[0]
indices_train2, indices_test2 = splits[1]
indices_train3, indices_test3 = splits[2]
indices_train4, indices_test4 = splits[3]

# Creating the four sets
x_train1, y_train1 = x_train[indices_train1], y_train[indices_train1]
x_test1, y_test1 = x_train[indices_test1], y_train[indices_test1]

x_train2, y_train2 = x_train[indices_train2], y_train[indices_train2]
x_test2, y_test2 = x_train[indices_test2], y_train[indices_test2]

x_train3, y_train3 = x_train[indices_train3], y_train[indices_train3]
x_test3, y_test3 = x_train[indices_test3], y_train[indices_test3]

x_train4, y_train4 = x_train[indices_train4], y_train[indices_train4]
x_test4, y_test4 = x_train[indices_test4], y_train[indices_test4]

x_train1=x_train1.astype('float32')
x_test1=x_test1.astype('float32')
x_train1=x_train1/255.0
x_test1=x_test1/255.0

x_train2=x_train2.astype('float32')
x_test2=x_test2.astype('float32')
x_train2=x_train2/255.0
x_test2=x_test2/255.0


x_train3=x_train3.astype('float32')
x_test3=x_test3.astype('float32')
x_train3=x_train3/255.0
x_test3=x_test3/255.0

x_train4=x_train4.astype('float32')
x_test4=x_test4.astype('float32')
x_train4=x_train4/255.0
x_test4=x_test4/255.0

s1=StratifiedShuffleSplit(n_splits=5, random_state=0, test_size=1/6)
train_index1, valid_index1 = next(s1.split(x_train1, y_train1))
x_valid1, y_valid1 = x_train1[valid_index1], y_train1[valid_index1]
x_train1, y_train1 = x_train1[train_index1], y_train1[train_index1]
print(x_train1.shape, x_valid1.shape, x_test1.shape)

s2=StratifiedShuffleSplit(n_splits=5, random_state=0, test_size=1/6)
train_index2, valid_index2 = next(s2.split(x_train2, y_train2))
x_valid2, y_valid2 = x_train2[valid_index2], y_train2[valid_index2]
x_train2, y_train2 = x_train2[train_index2], y_train2[train_index2]
print(x_train2.shape, x_valid2.shape, x_test2.shape)

s3=StratifiedShuffleSplit(n_splits=5, random_state=0, test_size=1/6)
train_index3, valid_index3 = next(s3.split(x_train3, y_train3))
x_valid3, y_valid3 = x_train3[valid_index3], y_train3[valid_index3]
x_train3, y_train3 = x_train3[train_index3], y_train3[train_index3]
print(x_train3.shape, x_valid3.shape, x_test3.shape)

model1 = tf.keras.models.Sequential()
model1.add(tf.keras.layers.BatchNormalization(input_shape=x_train1.shape[1:]))
model1.add(tf.keras.layers.Conv2D(64,(5,5), padding ='same', activation ='elu'))
model1.add(tf.keras.layers.MaxPooling2D(pool_size=(2,2), strides=(2,2)))
model1.add(tf.keras.layers.Dropout(0.25))


model1.add(tf.keras.layers.BatchNormalization(input_shape=x_train1.shape[1:]))
model1.add(tf.keras.layers.Conv2D(64,(5,5), padding ='same', activation ='elu'))
model1.add(tf.keras.layers.MaxPooling2D(pool_size=(2,2), strides=(2,2)))
model1.add(tf.keras.layers.Dropout(0.25))


model1.add(tf.keras.layers.BatchNormalization(input_shape=x_train1.shape[1:]))
model1.add(tf.keras.layers.Conv2D(64,(5,5), padding ='same', activation ='elu'))
model1.add(tf.keras.layers.MaxPooling2D(pool_size=(2,2), strides=(2,2)))
model1.add(tf.keras.layers.Dropout(0.25))

model1.summary()

model2 = tf.keras.models.Sequential()
model2.add(tf.keras.layers.BatchNormalization(input_shape=x_train2.shape[1:]))
model2.add(tf.keras.layers.Conv2D(64,(5,5), padding ='same', activation ='elu'))
model2.add(tf.keras.layers.MaxPooling2D(pool_size=(2,2), strides=(2,2)))
model1.add(tf.keras.layers.Dropout(0.25))


model2.add(tf.keras.layers.BatchNormalization(input_shape=x_train2.shape[1:]))
model2.add(tf.keras.layers.Conv2D(64,(5,5), padding ='same', activation ='elu'))
model2.add(tf.keras.layers.MaxPooling2D(pool_size=(2,2), strides=(2,2)))
model2.add(tf.keras.layers.Dropout(0.25))


model2.add(tf.keras.layers.BatchNormalization(input_shape=x_train2.shape[1:]))
model2.add(tf.keras.layers.Conv2D(64,(5,5), padding ='same', activation ='elu'))
model2.add(tf.keras.layers.MaxPooling2D(pool_size=(2,2), strides=(2,2)))
model2.add(tf.keras.layers.Dropout(0.25))

model2.summary()

from keras.layers.merging.concatenate import Concatenate

# Assuming you have two models: model1 and model2
output1 = model1.get_layer('dropout_2').output
output2 = model2.get_layer('dropout_5').output

# Concatenate the two output layers
concatenated = Concatenate()([output1, output2])

print(concatenated.shape)

model2.add(tf.keras.layers.BatchNormalization(input_shape=concatenated.shape[1:]))
model2.add(tf.keras.layers.Conv2D(64,(5,5), padding ='same', activation ='elu'))
model2.add(tf.keras.layers.MaxPooling2D(pool_size=(2,2), strides=(2,2)))
model2.add(tf.keras.layers.Dropout(0.25))

model2.add(tf.keras.layers.Flatten())
model2.add(tf.keras.layers.Dense(256))
model2.add(tf.keras.layers.Activation('elu'))
model2.add(tf.keras.layers.Dropout(0.5))
model2.add(tf.keras.layers.Dense(10))
model2.add(tf.keras.layers.Activation('softmax'))
model2.summary()

# Compile the model
model2.compile(
    optimizer=tf.keras.optimizers.Adam(learning_rate=1e-3),
    loss=tf.keras.losses.SparseCategoricalCrossentropy(),
    metrics=[tf.keras.metrics.SparseCategoricalAccuracy()]
)

model2.fit(
    x=x_train2,
    y=y_train2,
    validation_data=(x_test2, y_test2),
    batch_size=32,
    epochs=2
)

# Evaluate the model on the test data
evaluation2 = model2.evaluate(x=x_test2, y=y_test2)

# Print the evaluation results
loss = evaluation2[0]
accuracy = evaluation2[1]
print(f"Test Loss2: {loss:.4f}")
print(f"Test Accuracy2: {accuracy:.4f}")

model3 = tf.keras.models.Sequential()
model3.add(tf.keras.layers.BatchNormalization(input_shape=x_train3.shape[1:]))
model3.add(tf.keras.layers.Conv2D(64,(5,5), padding ='same', activation ='elu'))
model3.add(tf.keras.layers.MaxPooling2D(pool_size=(2,2), strides=(2,2)))
model3.add(tf.keras.layers.Dropout(0.25))

model3.add(tf.keras.layers.BatchNormalization(input_shape=x_train3.shape[1:]))
model3.add(tf.keras.layers.Conv2D(64,(5,5), padding ='same', activation ='elu'))
model3.add(tf.keras.layers.MaxPooling2D(pool_size=(2,2), strides=(2,2)))
model3.add(tf.keras.layers.Dropout(0.25))


model2.add(tf.keras.layers.BatchNormalization(input_shape=x_train3.shape[1:]))
model2.add(tf.keras.layers.Conv2D(64,(5,5), padding ='same', activation ='elu'))
model2.add(tf.keras.layers.MaxPooling2D(pool_size=(2,2), strides=(2,2)))
model2.add(tf.keras.layers.Dropout(0.25))

model3.summary()

# Compile the model
model1.compile(
    optimizer=tf.keras.optimizers.Adam(learning_rate=1e-3),
    loss=tf.keras.losses.SparseCategoricalCrossentropy(),
    metrics=[tf.keras.metrics.SparseCategoricalAccuracy()]
)